{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e42adcd",
   "metadata": {},
   "source": [
    "# Graph9: Agent4\n",
    "## Task\n",
    "    - Create an AI Agentic System that can speed up drafting of documents, emails etc. \n",
    "    - Integrate Human-Ai integration. \n",
    "    - Feedback should be provided by a human and feeback implemented by the Ai until satisfaction by human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6352bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict,  Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage, ToolMessage, SystemMessage, HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f7c4a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxdb\\AppData\\Local\\Temp\\ipykernel_2100\\1766644248.py:9: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Open router server/api\n",
    "api_key = os.getenv(\"open_router_api_key\")\n",
    "model_name=\"meta-llama/llama-4-maverick:free\"\n",
    "model_name2 = \"google/gemini-2.0-flash-001\"\n",
    "base_url=\"https://openrouter.ai/api/v1\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "  openai_api_key=api_key,\n",
    "  openai_api_base=base_url,\n",
    "  model_name=model_name2,\n",
    ")\n",
    "\n",
    "# Google ai\n",
    "llm2 = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d5adda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a global variable to Grant tool access to state\n",
    "document_content = \"\"\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a186d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def update(content: str) -> str:\n",
    "    \"\"\"Updates the document with the provided content.\"\"\"\n",
    "    global document_content\n",
    "    document_content = content\n",
    "    return f\"Document has been updated successfully! The current content is:\\n{document_content}\"\n",
    "\n",
    "@tool\n",
    "def save(filename: str) -> str:\n",
    "    \"\"\"Save the current document to a text file and finish the process.\n",
    "\n",
    "    Args:\n",
    "        filename: Name for the text file.\n",
    "    \"\"\"\n",
    "    global document_content\n",
    "\n",
    "    if not filename.endswith(\".txt\"):\n",
    "        filename = f\"{filename}.txt\"\n",
    "    \n",
    "    try:\n",
    "        with open(filename, \"w\") as file:\n",
    "            file.write(document_content)\n",
    "        print(f\"\\nDocument has been saved to: {filename}\")\n",
    "        return f\"Document has been saved successfully to '{filename}'.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error saving document: {str(e)}\"\n",
    "    \n",
    "tools = [update, save]\n",
    "model = llm2.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872fb75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_agent(state:AgentState) -> AgentState:\n",
    "    system_prompt = SystemMessage(content=f\"\"\"\n",
    "    You are Drafter, a helpful writing assistant. You are going to help the user update and modify documents.\n",
    "                                    \n",
    "    - If the user wants to update or modify content, use the \"update\" tool with the complete updated content.\n",
    "    - If the user wants to save and finish, you need to use the \"save\" tool.\n",
    "    - Make sure to always show the current document state after modifications.\n",
    "\n",
    "    The current document content is: {document_content}                                  \n",
    "    \"\"\")\n",
    "\n",
    "    if not state[\"messages\"]:\n",
    "        user_input = \"I'm ready to help you update a document. What would you like to create?\"\n",
    "        user_message = HumanMessage(content=user_input)\n",
    "    else:\n",
    "        user_input = input(\"\\nWhat would you like to do with the document? \")\n",
    "        print(f\"\\n🤵 USER: {user_input}\")\n",
    "        user_message = HumanMessage(content=user_input)\n",
    "\n",
    "    all_messages = [system_prompt] + list(state[\"messages\"]) + [user_message]\n",
    "    response = model.invoke(all_messages)\n",
    "    print(f\"\\n🤖 AI: {response.content}\")\n",
    "\n",
    "    if hasattr(response, \"tool_calls\") and response.tool_calls:\n",
    "        print(f\"🛠️ USING TOOLS: {[tc[\"name\"] for tc in response.tool_calls]}\")\n",
    "\n",
    "    return {\"messages\": list(state[\"messages\"] + [user_message, response])}\n",
    "\n",
    "def should_loop(state: AgentState) -> str:\n",
    "    \"\"\"Determine if we should continue or end the conversation\"\"\"\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    if not messages:\n",
    "        return \"continue\"\n",
    "    \n",
    "    for message in reversed(messages):\n",
    "        if(isinstance(message, ToolMessage) and \"saved\" in message.content.lower() and \"document\" in message.content.lower()):\n",
    "            return \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4331518d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8ea321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaf391a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a209104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcdaeef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
