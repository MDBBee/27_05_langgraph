{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fbb100f",
   "metadata": {},
   "source": [
    "## ðŸ’« MultiTurn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "115de3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, START, END, add_messages\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "from langgraph.types import Command, interrupt\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c7579ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "class State(TypedDict):\n",
    "    linkedIn_topic: str\n",
    "    generated_post: Annotated[List[str], add_messages]\n",
    "    human_feedback: Annotated[List[str], add_messages]\n",
    "\n",
    "GENERATED_POST = \"generated_post\"\n",
    "HUMAN_FEEDBACK = \"human_feedback\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36948d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(state:State):\n",
    "    \"\"\"\n",
    "    Using the LLM to generate a Linkedin post with human feedback integration.\n",
    "    \"\"\"\n",
    "    print(\"[model] Generating content\")\n",
    "    linkedIn_topic = state[\"linkedIn_topic\"]\n",
    "    feedback = state[\"human_feedback\"] if \"human_feedback\" in state else [\"No Feedback yet\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        LinkedIn Topic: {linkedIn_topic}\n",
    "        Human Feedback: {feedback[-1] if feedback else \"No feedback yet\"}\n",
    "\n",
    "        Generate a structured and well-written LinkedIn post based on the above provided topic.\n",
    "        Consider previous human feedback to refine the response.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=\"You are an expert LinkedIn content writer\"), \n",
    "        HumanMessage(content=prompt)\n",
    "        ])\n",
    "    \n",
    "    generated_post = response.content\n",
    "    print(f\"[node_name: mode] Generated post:\\n{generated_post}\")\n",
    "\n",
    "    return {GENERATED_POST: [AIMessage(content=generated_post)], HUMAN_FEEDBACK: feedback}\n",
    "\n",
    "def human_node(state:State):\n",
    "    \"\"\" Human node for review purposes \"\"\"\n",
    "    print(\"\\n[node: human_node] awaiting feedback from you...\")\n",
    "\n",
    "    generated_post = state[\"generated_post\"]\n",
    "    feedback = interrupt({\n",
    "        \"post_generated_by_llm\": generated_post,\n",
    "        \"message\": \"Provide feedback or type 'done' to accept the post\"\n",
    "    })\n",
    "\n",
    "    print(f\"[human_node] Received human feedback: {feedback}\")\n",
    "\n",
    "    if feedback.lower() == \"done\":\n",
    "        return Command(goto=\"end_node\", update={HUMAN_FEEDBACK: state[\"human_feedback\"] + [\"Finalised\"]})\n",
    "\n",
    "    return Command(goto=\"model\", update={HUMAN_FEEDBACK: state[\"human_feedback\"] + [feedback]})\n",
    "\n",
    "\n",
    "def end_node(state:State):\n",
    "    \"\"\" Node for printing ouputs \"\"\"\n",
    "    print(f\"\\n[node: end_node] Process finished\")\n",
    "    print(f'\\nFinal Generated Post, {state[\"generated_post\"][-1]}')\n",
    "    print(f'\\nFinal Human Feedback, {state[\"human_feedback\"]}')\n",
    "    return {GENERATED_POST: state[\"generated_post\"], HUMAN_FEEDBACK: state[\"human_feedback\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7a2e57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "\n",
    "graph = StateGraph(State)\n",
    "graph.add_node(\"model\", model)\n",
    "graph.add_node(\"human_node\", human_node)\n",
    "graph.add_node(\"end_node\", end_node)\n",
    "\n",
    "graph.add_edge(START, \"model\")\n",
    "graph.add_edge(\"model\", \"human_node\")\n",
    "\n",
    "graph.set_finish_point(\"end_node\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = graph.compile(checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\":{\n",
    "    \"thread_id\" : uuid.uuid4()\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ef96a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_topic = input(\"Enter your topic: \")\n",
    "initial_state = {\n",
    "    \"linkedIn_topic\": linkedin_topic,\n",
    "    GENERATED_POST: [],\n",
    "    HUMAN_FEEDBACK: []\n",
    "}\n",
    "\n",
    "for chunck in app.stream(initial_state, config):\n",
    "    print(\"ðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥CHUNCK\", chunck)\n",
    "    for node_id, value in chunck.items():\n",
    "        # On \"interrupt\", continuosly ask for feedback from a human\n",
    "        if (node_id == \"__interrupt__\"):\n",
    "            while True:\n",
    "                feedback = input(\"Provide feedback or type 'done' to accept the post as is! : \")\n",
    "\n",
    "                # Resume graph execution with feedback consideration\n",
    "                app.invoke(Command(resume=feedback), config)\n",
    "\n",
    "                # feedback == \"done\", break loop\n",
    "                if feedback.lower() == \"done\":\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f331722a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
